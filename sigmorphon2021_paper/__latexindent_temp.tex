% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{tipa}
\usepackage{amsmath}
\usepackage{amssymb}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Were We Already There? Applying Minimal Generalization to the SIGMORPHON-UniMorph Shared Task on Cognitively Plausible Morphological Inflection}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
  Colin Wilson$^1$ \qquad\qquad\qquad Jane S.Y. Li$^{1,2}$ \\
  $^1$Johns Hopkins University \qquad $^2$Simon Fraser University \\
  \texttt{colin.wilson@jhu.edu} \quad \texttt{sli213@jhu.edu}}

%\author{Colin Wilson$^1$ \\
%  $^1$Johns Hopkins University \\
%  \texttt{colin.wilson@jhu.edu} \\
%  \And
%  Jane S.Y. Li$^{1,2}$ \\
%  $^2$Simon Fraser University \\
%  \texttt{sli213@jhu.edu}}

\begin{document}
\maketitle
\begin{abstract}
Morphological rules with various levels of specificity can be learned from examples by recursive application of minimal generalization \citep{albright-hayes-2002-modeling, albright2003}. A model that learns rules solely through minimal generalization was used to predict average human wug-test ratings from German, English, and Dutch in the SIGMORPHON-UniMorph 2021 Shared Task, with competitive results. Some formal properties of the minimal generalization operation were proved, justifying the model's learning algorithm and allowing the resulting rule sets to be substantially pruned. An automatic method was developed to create wug-test stimuli for future experiments that investigate whether the model's morphological generalizations are too minimal.
\end{abstract}

\section{Introduction}

In a landmark paper, \citet{albright2003} proposed a model that learns morphological rules by recursive \textbf{minimal generalization} from lexeme-specific examples (\emph{e.g.}, \textipa{I} $\to$ \textipa{2} / \textipa{st} \underline{\ \ \ } \textipa{N} for \emph{sting $\sim$ stung} and \textipa{I} $\to$ \textipa{2} / \textipa{fl} \underline{\ \ \ } \textipa{N} for \emph{fling $\sim$ flung} generalized to  \textipa{I} $\to$ \textipa{2} / X [$-$syllabic, $+$coronal, $+$anterior, \ldots] \underline{\ \ \ } \textipa{N}).\footnote{The square brackets contain all of the the shared phonological feature specifications of /t/ and /l/, which in the system used here are $[$$-$syllabic, $+$consonantal, $-$nasal, $-$spread.gl, $-$LABIAL, $-$round, $-$labiodental, $+$CORONAL, $+$anterior, $-$distributed, $-$strident, $-$DORSAL$]$.} The model was presented more formally in \citet{albright-hayes-2002-modeling}, along with evidence that the rules it learns for the English past tense give a good account of native speakers' productions and ratings in wug-test experiments (\emph{e.g.} judgments that \textit{splung} is quite acceptable as the past tense of the novel verb \textit{spling}). In addition to providing further analysis of the experimental data, \citet{albright2003} compared their proposal with early connectionist models of morphology \citep[\emph{e.g.,}][]{plunkett1999} and an analogical or `family resemblance' model inspired by research on psychological categories \citep{nakisa2001}.

Along with \citet{albright2002}, which presents a parallel treatment of Italian inflection, Albright \& Hayes's study of the English past tense is a paradigm example of theory-driven, multiple-methodology, open and reproducible research in cognitive science.\footnote{Albright \& Hayes released both the results of their wug-test experiments and an implementation of their model (see \url{http://www.mit.edu/~albright/mgl/} and \url{https://linguistics.ucla.edu/people/hayes/RulesVsAnalogy/index.html}). An impediment to large-scale simulation with the model is that it runs from a GUI interface only. As part of the present project, we have added a command line interface to the original source code and converted the English input files to a more user-friendly format (available on request). We are aware of one other implementation of the minimal generalization model, due to Jo\~{a}o Ver\'issimo, but this was unavailable at the time of our study (\url{https://www.jverissimo.net/resources}).} Their model has enduring significance for the study of morphological learning and productivity in English \citep[\emph{e.g.,}][]{racz-etal-2014-rules, racz2020, corkery-etal-2019-yet} and many other languages (\emph{e.g.,} Hijazi Arabic: \citealt{ahyad2019}; Japanese: \citealt{oseki-etal-2019-inverting}; Korean: \citealt{albright2009}; Navajo: \citealt{albright2006}; Portuguese: \citealt{verissimo2014}; Russian: \citealt{kapatsinski2010}; Tgdaya Seediq: \citealt{kuo2020}; Spanish: \citealt{albright2003}; Swedish: \citealt{strik2014}).


In this study, we applied a partial reimplementation of the \citet{albright-hayes-2002-modeling, albright2003} model to wug-test rating data from three languages (German, English, and Dutch) collected for the SIGMORPHON-UniMorph 2021 Shared Task. Our version of the model is based purely on minimal generalization of morphological rules, as described in \S3.1 of \citet{albright-hayes-2002-modeling} and reviewed below. It does not include additional mechanisms for learning phonological rules, and expanding or reigning in morphological rules, that were part of the original model \citep[see][\S3.3 - \S3.7]{albright-hayes-2002-modeling}. We think it is worthwhile to consider minimal generalization on its own, with the other mechanisms ablated, as borne out by our competitive results on the shared task.

\subsection{Outline}

In \S\ref{sec:mingen} we review the definition of minimal generalization proposed by Albright \& Hayes and prove a number of original results about the operation and its recursive application in learning rules. We also define a generality relation that can be used to prune insufficiently broad rules without affecting the model's predictions. In \S\ref{sec:system} we describe how we preprocessed the shared task training data and generated predicted wug-test ratings, and report our results on the  task. We briefly summarize our findings in \S\ref{sec:summary} and conclude by discussing a novel method for constructing wug items that can be used in future empirical tests of minimal generalization and other approaches to morphological learning.

\section{Minimal Generalization}
\label{sec:mingen}

\subsection{Inputs}

The model takes as input a set of wordform pairs, one per lexeme, that instantiate the same morphological relationship in a language. In simulations of English past tense formation, these are pairs of bare verb stems and past tense forms such as $\langle$$\rtimes$\textipa{wOk}$\ltimes$, $\rtimes$\textipa{wOkt}$\ltimes$$\rangle$, $\langle$$\rtimes$\textipa{tOk}$\ltimes$, $\rtimes$\textipa{tOkt}$\ltimes$$\rangle$, $\langle$$\rtimes$\textipa{stIN}$\ltimes$, $\rtimes$\textipa{st2N}$\ltimes$$\rangle$, $\langle$$\rtimes$\textipa{flIN}$\ltimes$, $\rtimes$\textipa{fl2N}$\ltimes$$\rangle$, and $\langle$$\rtimes$\textipa{k2t}$\ltimes$, $\rtimes$\textipa{k2t}$\ltimes$$\rangle$. Wordforms consist of phonological segments (here, in broad IPA transcription) delimited by special beginning and end of string symbols. The set $\Sigma$ of phonological segments for the language, and the set $\Sigma_{\#} = \Sigma \ \cup \ \{ \rtimes, \ltimes \}$, are provided to the model.

The model also requires a phonological feature specification for each of the symbols that appears in wordforms. We adopted a well-known feature system, augmenting it with orthogonal and distinct feature specifications for the delimiters $\rtimes$ and $\ltimes$.\footnote{The phonological features are available from Bruce Hayes's website (\url{https://linguistics.ucla.edu/people/hayes/120a/Index.htm\#features}). These features are all binary, with the possibility of underspecification, while Albright \& Hayes's original simulations made use of some multi-valued, scalar features. Alternative sources of binary feature systems that are compatible with our implementation include PHOIBLE \citep{moran2014} and PanPhon \citep{mortensen-etal-2016-panphon}.} $\Phi$ is the set of all (partial) specifications of the features and $\phi(x)$ gives the specifications of $x \in \Sigma_{\#}$.

\subsection{Base rules}

For each wordform pair, the model constructs a lexeme-specific morphological rule by first identifying the longest common prefix (lcp) of the wordforms excluding $\ltimes$ (\emph{i.e.}, the left-hand context $C$), then the longest common suffix from the remainder (the right-hand context $D$), and finally identifying the remaining symbols in the first ($A$) and second ($B$) wordform. The resulting rule is $A \to B / C \underline{\ \ \ } D$. The symbol $\varnothing \notin \Sigma_{\#}$ denotes the empty string in $A$ or $B$.\footnote{We instead use $\lambda \notin \Sigma_{\#}$ to stand for the empty string in left- and right- hand contexts. Our notation for strings of phonological segments generally follows \citet{chandlee2017} and research cited there.} To illustrate, the rule formed from $\langle$$\rtimes$\textipa{wOk}$\ltimes$, $\rtimes$\textipa{wOkt}$\ltimes$$\rangle$ has the components C = $\rtimes$\textipa{wOk}, D = $\ltimes$, A = $\varnothing$ and B = \textipa{t} (\emph{i.e.}, $\varnothing$ $\to$ \textipa{t} / $\rtimes$\textipa{wOk} \underline{\ \ \ } $\ltimes$). The rule for $\langle$$\rtimes$\textipa{k2t}$\ltimes$, $\rtimes$\textipa{k2t}$\ltimes$$\rangle$ is $\varnothing$ $\to$ $\varnothing$ / $\rtimes$\textipa{k2t} \underline{\ \ \ } $\ltimes$.

\subsection{Minimal Generalization}
\label{sec:mingenop}
Given any two base rules $R_1$ and $R_2$ that make the same change (A $\to$ B), the model forms a possibly more general rule by aligning and comparing their contexts. The minimal generalization operation, $R = R_1 \sqcap R_2$, carries over the common change of the two base rules and applies independently to their left-hand ($C_1$, $C_2$) and right-hand ($D_1$, $D_2$) contexts. For convenience, we define minimal generalization of the right-hand contexts. Minimal generalization of the left-hand contexts can be performed by reversing $C_1$ and $C_2$, applying the definition for right-hand contexts, and reversing the result.

The minimal generalization $D = D_1 \sqcap D_2$ is defined precedurally by first extracting the lcp $\sigma_{1\land2}$ of the two contexts and then operating on the remainders ($D_1'$, $D_2'$). If both $D_1'$ and $D_2'$ are empty then $D = \sigma_{1\land2}$. If one but not both of them are empty then $D = \sigma_{1\land2}X$, where $X \notin \Sigma_{\#}$ is a variable over symbol sequences (\emph{i.e.}, $X$ stands for $\Sigma_{\#}^*$). If neither is empty, then the operation determines whether their initial symbols have any shared features; for this purpose it is useful to consider $\phi(x)$ as a function from symbols to sets of feature-value pairs, so that common features are found by set intersection.

If there are no common features, $\phi_{1\cap2} = \emptyset$, then as before $D = \sigma_{1\land2}X$. Otherwise, the set of common features $\phi_{1\cap2} \neq \emptyset$ is appended to $\sigma_{1\land2}$, the first symbol is removed from $D_1'$ and $D_2'$, and the operation applies to the remainders. If both remainders are empty then $D = \sigma_{1\land2} \phi_{1\cap2}$, otherwise $D = \sigma_{1\land2}\phi_{1\cap2}X$.

In summary, the generalized right-hand context $D$ consists of the longest common prefix shared by $D_1$ and $D_2$, followed by a single set of shared features (if any), followed by $X$ in case there are no shared features or one context is longer than the other. With the change and generalized left-hand context $C$ determined as noted above, the result of applying minimal generalization to the two base rules is $R = A \to B / C \ \underline{\ \ \ } \ D$.\footnote{There could be a small difference between our definition of context generalization and that in \citet{albright-hayes-2002-modeling}, hinging on whether the empty feature set is allowed in rules. In our definition, $\phi_{1\cap2} = \emptyset$ is replaced by the variable $X$. It is possible that the original proposal intended for empty and non-empty feature sets to be treated alike. The definitions can diverge when applied to contexts that are of identical length and share all but the last (resp. first) segments, in which case our version would result in a broader rule.}

\subsection{Recursive Minimal Generalization}

Let $\mathcal{R}_1$ be the set of base rules (one per wordform pair in the input data) and $\mathcal{R}_2$ be the set containing all of the base rules and the result of applying minimal generalization to each eligible pair of base rules. While the rules of $\mathcal{R}_2$ have greater collective scope, they are nevertheless unlikely to account for the level of morphological productivity shown by native speakers. For example, English speakers can systematically rate and produce past tense forms of novel verbs that contain unusual segment sequences, such as \emph{ploamf} \textipa{/ploUmf/} \citep[\emph{e.g.},][]{prasada1993}. Albright \& Hayes propose to apply minimal generalization recursively and demonstrate that this can yield rules that are highly general (\emph{e.g.}, in our notation, $\varnothing$ $\to$ \textipa{t} / X [-voice] \underline{\ \ \ } $\ltimes$).

In the original proposal, recursive minimal generalization was defined only for pairs that include one base rule; it was conjectured that no additional generalizations could result from dropping this restriction. Here we define the operation for any two right-hand contexts $D_1, D_2 \in \Sigma_{\#}^*(\Phi)(X)$. As before, only rules that make the same change are eligible for generalization and the operation applies to left-hand contexts under reversal.

The revised definition of $D = D_1 \sqcap D_2$ is identical to that given above except that we must consider input contexts that contain feature sets and $X$ (which previously could occur only in outputs). As before, we first identify the lcp of symbols from $\Sigma_{\#}$ in the two contexts, $\sigma_{1\land 2}$, and then operate on the remainders ($D_1', D_2'$). If both $D_1'$ and $D_2'$ are empty then $D = \sigma_{1\land2}$. If one but not both of them are empty then $D = \sigma_{1\land2}X$. If both are non-empty then their initial elements are either symbols in $\Sigma_{\#}$, feature sets in $\Phi$, or $X$. Replace any initial symbol $x \in \Sigma_{\#}$ with its feature set $\phi(x)$, extend the function $\phi$ so that $\phi(X) = \emptyset$, and compute the unification $\phi_{1\cap2}$ of the initial elements. The rest of the definition is unchanged (see the end of \S\ref{sec:mingenop}). 

By construction, the contexts that result from this operation are also in $\Sigma_{\#}^*(\Phi)(X)$ (\emph{i.e.}, no ordinary symbol can occur after a feature set, there is at most one feature set, $X$ can only be a terminal element, etc.). Therefore, the revised definition supports the application of minimal generalization to its own products. Let $\mathcal{R}_{k}$ be the set of rules containing every member of $\mathcal{R}_{k-1}$ and the result of applying minimal generalization to each eligible pair of rules in $\mathcal{R}_{k-1}$ (for $k > 1$). In principle, there is an infinite sequence of rules set related by inclusion $\mathcal{R}_1 \subseteq \mathcal{R}_2 \subseteq \mathcal{R}_3 \cdots$. In practice, the equality becomes strict after a small number of iterations of minimal generalization (typically 6-7), at which point there are no more rules to be found.

\subsection{Completeness}

Having defined minimal generalization for arbitrary contexts (as allowed by the model), we can revisit the conjecture that nothing is lost by restricting the operation to pairs at least one of which is a base rule. This is a practical concern, as the number of base rules is a constant determined by the input data while the number of generalized rules can increase exponentially.

Conceptually, each rule learned by unrestricted minimal generalization has a (possibly non-unique) `history' of base rules from which it originated. A base rule $R \in \mathcal{R}_1$ has the history $\{ R \}$. A rule in $R \in \mathcal{R}_2$ has the history $\{ R_1, R_2 \}$ containing the two base rules from which it derived. In general, the history of each rule in $\mathcal{R}_k$ is the union of the histories of two rules in $\mathcal{R}_{k-1}$ ($k > 1$).

Because all rules are learned `bottom-up' in this sense, the conjecture can be proved by showing that the minimal generalization operation is associative; we also show that it is commutative --- both properties inherited from equality, lcp, set intersection, and other more primitive ingredients. As before, we explicitly consider right-hand contexts, from which parallel results for left-hand contexts and entire rules follow immediately. It follows that any rule $R$ can be replaced, for the purposes of minimal generalization, with the base rules in its history (in any order). 

\textbf{Commutative}. Let $D = D_1 \sqcap D_2$ for any $D_1, D_2 \in \Sigma_{\#}^*(\Phi)(X)$. We prove by construction that $D$ is also equal to $D_2 \sqcap D_1$. The lcp of elements from $\Sigma_{\#}$ is the same regardless of the order of the contexts ($\sigma_{1\land 2} = \sigma_{2\land 1}$) as are the remainders ($D_1'$ and $D_2'$). If both remainders are empty, then the result of minimal generalization is $\sigma_{1\land 2} = \sigma_{2\land 1}$. If one but not both of them are empty then the result is $\sigma_{1\land 2}X = \sigma_{2\land 1}X$; note that $X$ appears regardless of which context is longer. If both are non-empty then we ensure that their initial elements are (possibly empty) feature sets and take their intersection, which is order independent: $\phi_{1\cap 2} = \phi_{2\cap 1}$. If $\phi_{1\cap 2} = \emptyset$ then the result is $\sigma_{1\land 2}X = \sigma_{2\land 1}X$. Otherwise, the initial elements are removed and the operation continues to the remainders. If both remainders are empty the result is $\sigma_{1\land 2}\phi_{1\cap 2} = \sigma_{2\land 1}\phi_{2\cap 1}$, otherwise it is the same expressions terminated by $X$.

\textbf{Associative}. Let $D = (D_1 \sqcap D_2) \sqcap D_3$ for any $D_1, D_2, D_3 \in \Sigma_{\#}^*(\Phi)(X)$. We prove by construction that $D$ is equal to $E = D_1 \sqcap (D_2 \sqcap D_3)$. Let $\sigma$ be the longest prefix of symbols from $\Sigma_{\#}$ in $D$. Because $\sigma$ occurs in $D$ iff it is the lcp of this type in $(D_1 \sqcap D_2)$ and $D_3$, it must be a prefix of each of $D_1, D_2, D_3$ and the longest such prefix in at least one of them. It follows that $\sigma$ is also the lcp of symbols from $\Sigma_{\#}$ in $D_1$ and $(D_2 \sqcap D_3)$. Therefore, $D$ and $E$ both begin with $\sigma$. We now remove the prefix $\sigma$ from all of the input contexts and consider the remainders $D'_1, D'_2, D'_3$.

If all of the remainders are empty, then $D = E = \sigma$. If all but one of them are empty, then $D = E = \sigma X$.\footnote{If $D'_1$ or $D'_2$ is the longest context, assume by commutativity that it is $D'_1$. The minimal generalizations are $(D'_1 \sqcap D'_2) = X$ and $X \sqcap D'_3$ = $X$, which gives the same result as $(D'_2 \sqcap D'_3) = \lambda$ and $D'_1 \sqcap \lambda = X$. Similar reasoning applies if $D'_3$ is the longest context.} If none of the remainder is empty, let $\phi_1, \phi_2, \phi_3$ be their (featurized) initial elements. The intersection of those elements is independent of grouping, $\phi = (\phi_1 \cap \phi_2) \cap \phi_3 = \phi_1 \cap (\phi_2 \cap \phi_3)$. If the intersection is empty then again $D = E = \sigma X$. If the intersection is non-empty then $D$ and $E$ both begin $\sigma\phi$. Finally, remove the initial elements of each of $D'_1, D'_2, D'_3$ and compare the lengths of the remainders to determine whether $X$ appears at the end of $D$ and $E$; this is independent of grouping along the same lines shown previously.

\textbf{Complete}. \qquad We now prove by induction that, for any $R \in \mathcal{R}_k$ and $R_1, R_2 \in \mathcal{R}_{k-1}$ ($k > 1$) such that $R = R_1 \sqcap R_2$, rule $R$ can also be derived by applying minimal generalization to $R_1$ and one or more base rules (\emph{i.e.}, the rules in the history of $R_2$).\footnote{We ignore rules that are carried over from $\mathcal{R}_{k-1}$ to $\mathcal{R}_{k}$.} For $R \in \mathcal{R}_2$ this is true by definition. For $R \in \mathcal{R}_3$, we have $R = R_1 \sqcap R_2 = R_1 \sqcap (R_{21} \sqcap R_{22}) = (R_1 \sqcap R_{21}) \sqcap R_{22}$, where $R_{21}$ and $R_{22}$ are base rules whose minimal generalization results is $R_2$. In general, suppose that the statement is true for $k-1 > 0$. Then it is also true for $k$ because $R \in \mathcal{R}_k$ can be derived by $R_1 \sqcap R_2 = R_1 \sqcap (\sqcap_{i=1}^{n} R_{2i}) = (((R_{1} \sqcap R_{21}) \sqcap R_{22}) \cdots \sqcap R_{2n})$ where $R_1, R_2 \in \mathcal{R}_{k-1}$ and each $R_{2i}$ is a base rule in the history of $R_2$.

These results validate the rule learning algorithm proposed by \citet{albright-hayes-2002-modeling} and used in our implementation. Any minimal generalization of two rules $R_1$ and $R_2$ allowed by the model can be derived from $R_1$ (or $R_2$) by recursive application of minimal generalization with one or more base rules.


\subsection{Relative generality}
\label{subsec:generality}

While not required for the minimal generalization operation itself, we define here a (partial) generality relation on rules. The definition uses the same notation as above and is employed in pruning rules after recursive minimal generalization has been applied (see \S\ref{subsec:pruning} below). 

Relative generality is defined only for rules $R_1$ and $R_2$ that make the same change. As usual, it is sufficient to consider the right-hand contexts $D_1$ and $D_2$ and then apply the same definition to the reversed left-hand contexts. Conceptually, context $D_2$ is at least as general as context $D_1$, $D_1 \sqsubseteq D_2$, iff the set of strings represented by $D_1$ is a subset of that represented by $D_2$ when both contexts are considered as regular expressions over $\Sigma_{\#}^*$. The procedural definition is complicated somewhat by $X$, which can appear at the end of either context.

Replace each symbol $x \in \Sigma_{\#}$ in $D_1$ or $D_2$ with its feature set $\phi(x)$, treat $X$ as equivalent to $\emptyset$, and let $|D|$ be the length of context $D$.Then $D_1 \sqsubseteq D_2$ iff (i) $|D_1| \geq |D_2|$ and $D_1[k] \subseteq D_2[k]$ for all $1 \leq k \leq |D_1|$, except when $|D_1| = |D_2| + 1$ and the last element of $D_1$ but not $D_2$ is $X$, or (ii) $|D_1| = |D_2| - 1$, $D_1[k] \subseteq D_2[k]$ for all $1 \leq k \leq |D_1|$, and the last element of $D_2$ is $X$. Context $D_2$ is strictly more general than $D_1$, $D_1 \sqsubset D_2$, iff $D_1 \sqsubseteq D_2$ and $D_2 \not\sqsubseteq D_1$. Rule $R_2$ is at least as general as $R_1$, $R_1 \sqsubseteq R_2$, iff $C_1 \sqsubseteq C_2$ and $D_1 \sqsubseteq D_2$; it is a strictly more general rule iff either of the context relations is strict.


\section{System Description and Results}
\label{sec:system}

Our system for the shared task preprocesed the input wordforms, learned rules with recursive minimal generalization, scored the rules in two ways, pruned rule that have no effect on the model's predictions, and  applied the remaining rules to wug forms in order to generated predicted ratings.

\subsection{Preprocessing}

The shared task provided space-separated broad IPA transcriptions of the training and wug wordforms (\emph{e.g.}, \textipa{s t I N}, \textipa{s t 2 N}, \textipa{w O k}, \textipa{w O k t}). As already mentioned, we added explicit beginning and end of string symbols. Because minimal generalization requires each wordform symbol to have a phonological feature specificiation, but some segments in the data lack entries in our feature chart, we further simplified or split the symbols as follows.

For German, we split the diphthongs \textipa{/a\textsubarch{i} a\textsubarch{u} o\textsubarch{i} i:@ e:@ E:@/} into their component vowels and additionally regularized \textipa{/\textsubarch{i} \textsubarch{u}/} to \textipa{/i u/}. For English, we split the diphthongs \textipa{/aI aU OI u:I/} into their components and \textipa{/3\textrhoticity/} into \textipa{/E \*r/}, simplified \textipa{/eI @U/} to \textipa{/e o/}, and regularized \textipa{/\s{m} \s{n} r \s{l} \~{O}/} to \textipa{/m n \*r l O/}. We also deleted all length marks \textipa{/:/} and instances of \textipa{/\super G/}. For Dutch, we split \textipa{/EI AU UI/} into their components.

Checking that all wordform symbols appear in a phonological feature chart is also useful for data cleaning. It helped us to identify a few thousand  Dutch wordforms containing `$+$' (indicating a Verb $+$ Preposition juncture), which we removed. And it caught an encoding error in which two distinct but perceptually similar Unicode symbols were used for /g/.

Two acknowledged limitations of the original version of the minimal generalization model, and our version, are relevant here. First, the model learns rules for individual morphological relations (\emph{e.g.}, mapping a bare stem to a past tense form), not for entire morphological systems jointly. Therefore, we retained from the preprocessed input data only the wordform pairs that instantiate the relations targeted by the shared task: formation of past participles in German \citep{clahsen1999} and past tenses in English and Dutch \citep{booij2019}.

Second, the model cannot learn sensible rules for circumfixes \citep[][\S5.2]{albright-hayes-2002-modeling}. This could be remedied by allowing the model to form rules that simultaneously make changes at both wordform edges, or by allowing it to apply multiple rules when mapping inputs to outputs. As a workaround, we simply removed the prefix \textipa{/g@-/} whenever it occured at the beginning of a German past participle (training or wug wordform).

\subsection{Rules}

Given the preprocessed and filter input data, a base rule was learned for each lexeme and then minimal generalization was applied recursively as in \S\ref{sec:mingen}. This resulted in tens of thousands of morphological rules for each of the three languages (see Table~\ref{results}).

A major goal of Albright \& Hayes was to learn rules that can construct outputs from inputs (as opposed to merely rating or selecting outputs that are generated by some other source). Their model achieved this goal, and a substantial portion of its original implementation was dedicated to rule application. We instead delegated the application of rules to a general purpose finite-state library (Pynini; \citealp{gorman-2016-pynini, gorman2021}), as follows.

Each component of a rule $A \to B / C \underline{\ \ \ } D$ was first converted to a regular expression over symbols in $\Sigma_{\#}$ by mapping any feature set $\phi \in \Phi$ to the disjunction of symbols that bear all of the specified features and deleting instances of $X$. Segments were then encoded as integers using a symbol table. Pynini provides a function \texttt{cdrewrite} that compiles rules in this format to finite-state transducers, a function \texttt{accep} for converting input strings to linear finite-state acceptors encoded with the same symbol table, a composition function \texttt{@} that applies rules to inputs yielding output acceptors, and the means to decode the output back to strings.\footnote{The technique of mapping feature matrices to disjunctions (\emph{i.e.}, natural classes) of segments and beginning/end symbols, and ultimately to disjunctions of integer ids, was also used in the finite-state implementation of \citet{hayes2008}. $X$ was deleted here because it occurs only at the beginning of left-hand contexts and at the end of right-hand contexts, both positions where Pynini's rule compiler implicitly adds $\Sigma_{\#}^*$. Pynini's implementation of finite-state automata wraps and extends OpenFst \citep{riley-etal-2009-openfst} and its rule compilation algorithm is due to \citet{mohri-sproat-1996-efficient}.}

\subsection{Scoring}

The \emph{score} of a rule is related of its accuracy on the training data. The simplest notion of score would be just accuracy: the number of training outputs that are correctly predicted by the rule (\emph{hits}), divided by the number of training inputs that meet the structural description of the rule (\emph{scope}). Albright \& Hayes propose instead to discount the scores of rules with smaller scopes, using a formula previously applied to linguistic rules by \citet{mikheev-1997-automatic}. Our implementation also includes this way of scoring rules, which Albright \& Hayes call \textit{confidence}.\footnote{The confidence formula has one free parameter, which we set to $\alpha = .55$ following \citet[][p. 127]{albright2003}.}

Because confidence imposes only a modest penalty on rules with small scopes, we also considered a score function of the form $score_{\beta} = hits / (scope + \beta)$, where $\beta$ is a non-negative discount factor (here, $\beta = 10$). A rules that is perfectly accurate and applies to just $5$ cases has high confidence ($.90$) but much lower score$_{10}$ ($.33$); one that applies perfectly to $1000$ cases has a near-maximal value ($> .99$) regardless of how the score is calculated. Clearly, these are only two of a wide range of score functions that could be explored.

\subsection{Pruning}
\label{subsec:pruning}

When applied to training data consisting of thousands of lexemes, recursive minimal generalization can produce tens of thousands of distinct rules. Albright \& Hayes mention but do not implement the possibility of pruning the rules on the basis of their generality and scores. We pursued this suggestion by first partitioning the set of all learned rules according to their change and imposing a partial order on each of the resulting subsets.

We ordered rules by generality (\S\ref{subsec:generality}), score, and length when expressed with features \citep{chomsky1968a}. Rule $R_2$ dominated rule $R_1$ in the order, $R_1 \prec R_2$ iff $R_2$ was at least as general as $R_1$ ($R_1 \sqsubseteq R_2$) and (i) $R_2$ had a higher score or (ii) the rules tied on score and $R_2$ was either strictly more general ($R_1 \sqsubset R_2$) or shorter. Dominated rules were pruned without affecting the predictions of the model, as we discuss next.

\subsection{Prediction}

Once rules have been learned by minimal generalization and scored, they can be used for multiple purposes: to generate potential outputs for input wordforms (by finite-state composition), to determine possible inputs for a given output wordform (by composition with the inverted transducer), and to assign scores to input/output mappings. Following Albright \& Hayes, we assume that the score of a mapping is taken from the highest-scoring rule(s) that could produce it. Rules neither `gang up' --- multiple rules cannot contribute to the score of a mapping --- nor do they compete --- rules that prefer different outputs for the same input do not detract from the score. When no rule produces a mapping, we assigned it the minimal score of zero.

As for the scoring function itself, many other possibilities could be considered. For example, rule scores could be normalized within or across changes, a type of competition that is inherent to probabilistic models. See \citet{albright2006} for a different kind of competition model in which rules learned by minimal generalization are weighted like conflicting constraints.

\subsection{Results}

Table~\ref{results} provides quantitative details of our simulations for the three morphological relations in the shared task. The AIC values were calculated with an evaluation script provided by the organizers, which compares average human ratings of output wordforms with ratings predicted by the model. (The values are not directly comparable across languages because the number of wug forms differed.)

We used whichever scoring method, confidence or score$_{10}$, achieved a better AIC value on the development wug data. For German and English, this was confidence; for Dutch it was score$_{10}$. Upon close inspection of the development data for English, we found it plausible that human participants had down-rated regular past tense forms of bare forms ending in coronal stops \textipa{/t d/} because these might appear to be `double past' inflections (\emph{e.g.}, \textipa{/vaInd@d/} for the stem \textipa{/vaInd/}, which has a rime \textipa{/aInd/} that is rare outside of past tense forms). Therefore, in generating predictions for the English wug test we added a penalty to the model score for such outputs. The magnitude of the penalty was fit by linear regression to the development data. As the development and test wugs were generated by different methods, addition of this factor could have had a detrimental effect on the model's performance. On the contrary, our model had the best AIC for the German and English test data and the best overall AIC (summed over the languages).

\begin{table*}
  \centering
  \begin{tabular}{l c c c c c}
    \hline
    Language  & Lexemes & Rules (all) & Rules (pruned) & 
    AIC (dev wugs) & AIC (test wugs) \\
    \hline
    German (past part.) & 3,417 & 31,562 & 3,629 & -127.6 & -135.0\\
    English (past) & 5,803 & 30,728 & 263 & -112.0 & -62.2\\
    Dutch (past) & 7,823 & 55,114 & 1,862 & -58.5 & -76.5\\
    \hline
  \end{tabular}
\caption{\label{results}Number of lexemes (wordform pairs) used for training, number of rules learned by minimal generalization (before and after pruning), and evaluation on average human wug-test ratings for each language. Lower AIC values indicate better match between model predictions and average human ratings.}
\end{table*}

% German (deu): verb stem tp past % 3417 train examples; 150 wug dev; 266 wug tst
% mingen0 learned 31,562 rules; 3630 rules after pruning
% dev eval (AIC): -127.5508, tst eval (AIC): -135.0 *lower is better*
% Clahsen 1999 Appendix A


% English (eng): verb stem to past-tense form
% 5803 train examples; 158 wug dev; 139 wug tst
% mingen0 learned 30,728 rules; 263 rules after pruning
% dev eval (AIC): -112.0, tst eval (AIC): -62.2 *lower is better*

% Dutch (nld): verb stem to past-tense form
% 7823 train examples; 122 wug dev; 166 wug tst
% mingen0 learned 55,114 rules; 1,862 rules after pruning
% dev eval (AIC): -58.50812, tst eval (AIC): -76.5 *lower is better*
% Booij 2019


\section{Summary and Future Directions}
\label{sec:summary}

We have described the minimal generalization operation for morphological rules as proposed by Albright \& Hayes and presented some new formal results on this operation. We have also described our partial implementation of their model, a pure minimal generalization learner, and applied it to wug-test data from three related languages. We conclude with some remarks on how our implementation could be extended and how the central concept of minimal generalization could be empirically tested in future behavioral experiments.

\subsection{Extensions}

The most obvious extension of the present study would be to compare our stripped-down model with the original one. For some of the additional mechanisms proposed by Albright \& Hayes this would be straightforward and we have alreay begun to do so; other modifications would require larger changes to the model and enhancements to the training data. For example, \citet[][\S3.4]{albright-hayes-2002-modeling} motivate a second generalization mechanism that creates \emph{cross-context} (or more jocularly Doppelg\"anger) rules: for each pair of rules $A \to B / C \underline{\ \ \ } D$ and $A \to B' / C' \underline{\ \ \ } D'$, their model adds $A \to B' / C \underline{\ \ \ } D$ and $A \to B / C' \underline{\ \ \ } D'$. This is a simple change to our implementation that, using the results derived in \S\ref{sec:mingen}, need only apply to base rules.

Learning phonological rules along with morphology, as in \citet[][\S3.3]{albright-hayes-2002-modeling}, would require the training data to contain lexeme frequencies. This is because the original implementation processes the training lexemes in order of descending frequency, ensuring that a phonological rule learned on the basis of one lexeme is consistent with all previous (\emph{i.e.}, higher frequency) training examples. We have not yet begun to explore this or alternative means of incorporating phonology into the model; this is an important extension because, as Albright \& Hayes demonstrate, learning fully general morphological rules requires taking into account the downstream effects of phonology. We have also not explored \emph{impugnment} \citep[][\S3.7]{albright-hayes-2002-modeling}, which unlike the other components of the model seeks to limit rather than expand upon minimal generalization.

% minimal generalization related to: size principle (Tenenbaum & Griffiths, 2001); least-general generalization; Anti-unification
%\citep{tenenbaum1999} % size principle

%\citep{plotkin1970} % least general generalization
% least general generalization; it's legacy in inductive logic programming (ILP), a connection also noted by Albright \& Hayes

\subsection{Near misses}

As the organizers of the shared task have emphasized, implemented models can be used not only to predict the results of experiments but also to generate stimuli. Ideally, stimulus items would be designed to test the core tenets of a single model or to probe systematic differences in prediction among models. As part of our implementation, we have developed an automatic method of selecting wug items to investigate a main concern about minimal generalization: namely, that by learning rules in a strictly bottom-up way it will \emph{undergeneralize}, predicting sharp contrasts in inflectional behavior on the basis of slight differences in form.

We illustrate our method with the English irregular pattern \textipa{I} $\to$ \textipa{2}, which attracted new members in the history of English and has elicited relatively high production rates and acceptability ratings in previous wug tests \citep[\emph{e.g.},][]{bybee1983, albright2003}. We extracted all of the onsets and rimes that appear in the bare forms of monosyllabic English verbs and freely combined them to create a large pool of possible stimulus items. We eliminated items that are real verbs, then shrunk the pool to those items that are one (segmental) edit away from some existing irregular verb that undergoes \textipa{I} $\to$ \textipa{2}. We further required each item to share its rime with at least one such irregular verb.\footnote{Studies of English irregular verbs have focused primarily on vowels and codas of monosyllables, though see \citet{bybee1983} on the potential role of onsets.}  All of the wugs in the final pool are highly similar, in this sense, to existing irregulars.

We then divided the pool into two sets: items that are within the scope of at least one \textipa{I} $\to$ \textipa{2} rule learned by minimal generalization (\emph{potential hits}), and items that are outside the scope of all such rules (\emph{near misses}). For the former, we recorded the highest-scoring applicable rule. We wanted to provide the model with the opportunity to form rules that were as broad as possible --- making it more difficult for us to find near misses --- and therefore implemented cross-context base rules as described earlier.\footnote{With this modification to the implementation, which was not used in previous sections, the total number of rules for the English past tense ballooned to 191,874. Even after pruning there were tens of thousands of rules (69,747) and 128 for just \textipa{I} $\to$ \textipa{2}. The majority of the rules have very low scores.}

Some of the potential hits and near misses are minimal pairs. For example, \textipa{/lIN/} ($.67$) and \textipa{/SIN/} ($.61$) could potentially undergo \textipa{I} $\to$ \textipa{2} rules with the indicated confidence values. But \textipa{/fIN/} and \textipa{/vIN/} are ineligible for the change according to the model (because no existing irregular verb of this type has a non-coronal fricative immediately before the vowel). Other differences in the onset can also dramatically affect the model's predictions: \textipa{/T\*rINk/} ($.88$) and \textipa{/glIN/} ($.67$) are potential hits but \textipa{/smINk/} and \textipa{/smIN/} are near misses. The second two are phonotactically challenged \citep{davis-1989-cross}, but are \textipa{/T\*r2Nk/} and \textipa{/gl2N/} far superior to \textipa{/sm2Nk/} and \textipa{/sm2N/} when the phonotactic acceptability of their bare forms is factored out?

The same procedure can be applied to any irregular (or indeed regular) change. For \textipa{i} $\to$ \textipa{Ept} (as in \emph{sleep} $\sim$ \emph{slept}), we find that the potential hits include \textipa{/gip/} ($.85$) and \textipa{/flip/} ($.73$, one of Albright \& Hayes's wug items) while \textipa{/fip/}, \textipa{/vip/}, \textipa{/nip/}, and \textipa{/snip/} are among the near misses. Would native English speakers rate the novel past form \textipa{/gEpt/} much higher than \textipa{/fEpt/}, as the model predicts? We look forward to future empirical tests of minimal generalization, along these lines and others, as part of the collective effort to find out where we are and how much further we have to go in cognitive modeling of inflection.

% "attributes of the prototype of this [irregular] class of verbs are:
% a final velar nasal (/N/ better than /Nk/)
% an initial consonant cluster that begins with /s/
% vowel /I/ (only in conjunction with the other two)

\section*{Acknowledgements}

We would like to thank the organizers for all of their work on the shared task. Special thanks to Adam Albright and Bruce Hayes for inspiring this study and for stimulating conversations on related topics over many years. The research presented here was partially supported by NSF grant BCS-1844780 to CW.



% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology, wilsonlisigmorphon2021, wilsonlisigmorphon2021extra}
\bibliographystyle{acl_natbib}

\appendix

%\section{Example Appendix}
%\label{sec:appendix}

%This is an appendix.

\end{document}

These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

The templates include the \LaTeX{} source of this document (\texttt{acl.tex}),
the \LaTeX{} style file used to format it (\texttt{acl.sty}),
an ACL bibliography style (\texttt{acl\_natbib.bst}),
an example bibliography (\texttt{custom.bib}),
and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

\section{Engines}

To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

\section{Preamble}

The first line of the file must be
\begin{quote}
\begin{verbatim}
\documentclass[11pt]{article}
\end{verbatim}
\end{quote}

To load the style file in the review version:
\begin{quote}
\begin{verbatim}
\usepackage[review]{acl}
\end{verbatim}
\end{quote}
For the final version, omit the \verb|review| option:
\begin{quote}
\begin{verbatim}
\usepackage{acl}
\end{verbatim}
\end{quote}

To use Times Roman, put the following in the preamble:
\begin{quote}
\begin{verbatim}
\usepackage{times}
\end{verbatim}
\end{quote}
(Alternatives like txfonts or newtx are also acceptable.)

Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
\begin{quote}
\begin{verbatim}
\setlength\titlebox{<dim>}
\end{verbatim}
\end{quote}
where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

\section{Document Body}

\subsection{Footnotes}

Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

\subsection{Tables and figures}

See Table~\ref{tab:accents} for an example of a table and its caption.
\textbf{Do not override the default caption sizes.}

\begin{table}
\centering
\begin{tabular}{lc}
\hline
\textbf{Command} & \textbf{Output}\\
\hline
\verb|{\"a}| & {\"a} \\
\verb|{\^e}| & {\^e} \\
\verb|{\`i}| & {\`i} \\ 
\verb|{\.I}| & {\.I} \\ 
\verb|{\o}| & {\o} \\
\verb|{\'u}| & {\'u}  \\ 
\verb|{\aa}| & {\aa}  \\\hline
\end{tabular}
\begin{tabular}{lc}
\hline
\textbf{Command} & \textbf{Output}\\
\hline
\verb|{\c c}| & {\c c} \\ 
\verb|{\u g}| & {\u g} \\ 
\verb|{\l}| & {\l} \\ 
\verb|{\~n}| & {\~n} \\ 
\verb|{\H o}| & {\H o} \\ 
\verb|{\v r}| & {\v r} \\ 
\verb|{\ss}| & {\ss} \\
\hline
\end{tabular}
\caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
\label{tab:accents}
\end{table}

\subsection{Hyperlinks}

Users of older versions of \LaTeX{} may encounter the following error during compilation: 
\begin{quote}
\tt\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
\end{quote}
This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

\subsection{Citations}

\begin{table*}
\centering
\begin{tabular}{lll}
\hline
\textbf{Output} & \textbf{natbib command} & \textbf{Old ACL-style command}\\
% \hline
% \citep{Gusfield:97} & \verb|\citep| & \verb|\cite| \\
% \citealp{Gusfield:97} & \verb|\citealp| & no equivalent \\
% \citet{Gusfield:97} & \verb|\citet| & \verb|\newcite| \\
% \citeyearpar{Gusfield:97} & \verb|\citeyearpar| & \verb|\shortcite| \\
% \hline
\end{tabular}
\caption{\label{citation-guide}
Citation commands supported by the style file.
The style is based on the natbib package and supports all natbib citation commands.
It also supports commands defined in previous ACL style files for compatibility.
}
\end{table*}

Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

\subsection{References}

\nocite{Ando2005,borschinger-johnson-2011-particle,andrew2007scalable,rasooli-tetrault-2015,goodman-etal-2016-noise,harper-2014-learning}

The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{custom}
\end{verbatim}
\end{quote}

You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
To include both the Anthology and your own .bib file, use the following instead of the above.
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{anthology,custom}
\end{verbatim}
\end{quote}

Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

\subsection{Appendices}

Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

\section{Bib\TeX{} Files}
\label{sec:bibtex}

Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section*{Acknowledgements}

This research was partially supported by NSF grant BCS-1844780.



% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology, sigmorphon2021, sigmorphon2021extra}
\bibliographystyle{acl_natbib}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is an appendix.

\end{document}
